# 4 October 2023 Discussion about the Translator Testing Framework

* **Max:** I don’t think we want to build anything for the Dashboard, but try and find an existing open-source solution for the frontend. I would like to work on the Harness and Test Cases.
* **Richard:** If I were to express a bias, I'd say I could work with Sierra on the "Test Suite Repository" including the Test Case schemata including Test Case Documentation Application (all of which I sense could be LinkML schema driven? Which is more Sierra's cup of tea but about which, Sierra and I have conversed a bit)
* **Max:** You’re imagining the Test Case Documentation Application as being a UI?
* **Richard:** I'm imagining the Test Case Documentation Application as being whatever kind of software that will make it brain-dead easy for SMEs, SMURFs and Jenn to capture the test cases, test assets, test personas and test case metadata, in a fully structured manner, without too much free text noise and conflicting use discretion.... as Sierra and I have noted, Jenn's discussions about test cases are a bit hard to pin down at times. The proposed (LinkML) Translator Test Schema is the key: enforcing discipline on test case capture. The "documentation application" could be more of a toolkit of approaches that appeal to the test case curators and test case sources: CLI scripts, simply CRUD web forms, ??? - the key is K.I.S.S. with little margin for user ambiguity. Not sure exactly what that would look like, but... our gut feeling is that is it needed. As for the right hand side of the testing framework, if we nail down the Testing Schema then the Test Runner's will have a predictable environment the "Get Test Cases" arrow will be well-defined and clean
* **Max:**  I guess I’m still a little unclear. Is this software the step that takes the test assets and makes them into the test cases and test suites and grabs all the metadata from the test assets?
* **Richard:** By software, you mean "Test Case Documentation Application/UI"? Then, no, not really. Imagine the following sequence: SME/SMURF/TACT feedback issues/Jenn/etc. nebulous test cases **=>** Test Case Documentation Application, which maybe includes interactive or batch CLI, or perhaps, simple web forms, taking as input (templated?) spreadsheets, Q&A or similar inputs  **=>** (LinkML Test Schema) formally structured Test Suite Repository "data" (i.e. Test Assets, Personas, Test Cases, associated metadata **=>** Test Case Data retrieval API (could just be Github based or an OpenAPI structured interface) **=>** Test Harness/Runners => (again, structured, perhaps "off-the-shelf") Test Report Database **=>** (off-the-shelf solution?) Test Manager API & information radiator.  I guess what I'm saying is that the Test Case Documentation Application creates the Test Cases/Assets etc. (note: asset and scenario reuse may be helpful here; or even, copy and paste scenarios, test cases, and assets with customization to create new instances of any of these items).  Note that what I'm calling 'test case metadata' may not be in the test assets. It's the software - CLI interactive, batch script, CRUD web form, or whatever - that captures nebulous test curator intent from all the available sources - and converts it into the well-structured and fully semantically constrained Test Asset/Case/Persona/Metadata data in Test Suite Repository "semantically constrained" == LinkML test schema constrained.  Sierra has a solid vision what is required there and LinkML has alot of the tooling for that.
On a slightly different slant, I guess we've all assumed that there could be multiple test runners reusing the same Test Suite repo data?
* **Max:** Ok, I think I understand it now. I’m just wondering if it is a “nice to have” at this point. The idea was to make the Assets spreadsheet accessible enough that SMEs could directly add to it, which I know is kind of a stretch.
* **Richard:** I think giving SME's (even Jenn) just a spreadsheet is going to be a headache. No problem using it as a starting point (with Jenn) for now but we'll need to provide some semantic guard rails before long.  Again, I think LinkML will ensure that the Test Schema is the guiding principle here LinkML can read spreadsheet data.
* **Max:** I completely agree. But a headache is doable if we’re just trying to get this off the ground. But at the same time, if you all think this is important enough, you can work on it.
* **Richard:** what LinkML does is constrain the ETL of such data to strong semantic expectations, which is needed here Sierra and I can take care of that portion, with feedback from Jenn, you and others, of course I can talk to Sierra about this now.
* **Max:** What are your thoughts on making all the test_data_location into Test Assets? My opinion is that we try and incorporate that test data into the new Test Assets and make SRI_Testing into another Runner? I think we could use both **One Hop** ("HopLite") Test Runner, as well as, a Biolink Model/TRAPI Compliance Compliance (reasoner-validator) TestRunner
* **Richard:** Sure! Bottom line here: we have several distinct test runners to plug into the new framework. We just need to 1) agree on a common test asset/case data model and test data access modality and 2) implement the test harness calling our target test runners.
* **Richard:** As it happens, the "HopLite" (I'll use this term for the one hop only part of SRI_Testing) actually dynamically generates test cases from test input edges
I guess the test input edges are a very lightweight kind of 'test asset', a much more limited in concept than Jenn's full test asset idea. HopLite wouldn't necessarily grab its test cases from the Test Suite repository. In fact, in a manner of speaking, the HopLite defines its test suites based on the intersection of TRAPI component (with given TRAPI version and Biolink Model release), and a half dozen One Hop test (encoded as template methods taking test edges and forming a TRAPI query). That makes it a bit of a weird animal in the new proposed framework... I guess this is another expression of "one size doesn't fit all" thought we've all had. I mean, Jenn's single test asset-to-cases versus Andrew Su's batch file (test asset/cases?) versus (now) HopLite test runner dynamic test cases.
* **Max:** I think that all Runners should take test input from the Harness to generate their specific tests. The HopLite Runner could still take the same inputs as other Runners, but just format its query differently? Maybe TRAPI version and Biolink Model version are part of the Test Asset Metadata? Or Test Case data?
* **Richard:** Agreed.
